{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcmvX5UgiU4o"
      },
      "source": [
        "## Importation de bibliothèques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BNIzLVCx1Eb"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzCoaIkwbU11"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import spacy\n",
        "import en_core_web_sm\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from textblob import TextBlob\n",
        "from gensim.models import Word2Vec\n",
        "from sentence_transformers import SentenceTransformer, util"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCUlzNLGgjjn"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ze4ktBl5i2e-"
      },
      "source": [
        "## Extraction et Préparation de Données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LcqEGnmRba4X"
      },
      "outputs": [],
      "source": [
        "#   Fonction pour extraire le texte d'une page web\n",
        "def get_text_from_url(url):\n",
        "\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Vérifier si la requête a réussi : Un code 200 signifie que la requête a réussi ( l'erreur 404 signifie que la page n'a pas été trouvée)\n",
        "    if response.status_code == 200:\n",
        "        # Parser le contenu HTML de la page\n",
        "        soup = BeautifulSoup(response.content, 'lxml')\n",
        "\n",
        "        # Extraire tout le texte à l'intérieur des balises <p> (paragraphes)\n",
        "        paragraphs = soup.find_all('p')\n",
        "        text = \" \".join([para.get_text() for para in paragraphs])\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "# Prétraitement du texte\n",
        "def preprocess_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = nltk.word_tokenize(text.lower())\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word.isalnum() and word not in stop_words]\n",
        "    return ' '.join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QZB93_ccYqg",
        "outputId": "4f196723-56c8-4cdd-9dd1-60a827a82997"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extrait du document 1 :\n",
            "The era of generative artificial intelligence (AI)...\n",
            "\n",
            "Extrait du document 2 :\n",
            "Agree & Join LinkedIn\n",
            "             \n",
            "      By click...\n"
          ]
        }
      ],
      "source": [
        "# URL des deux documents fournis dans le mail\n",
        "url1 = \"https://english.elpais.com/technology/2024-07-16/artificial-intelligence-is-already-an-environmental-problem.html\"\n",
        "url2 = \"https://www.linkedin.com/pulse/ai-environment-how-artificial-intelligence-helping-save-planet\"\n",
        "\n",
        "text1 = get_text_from_url(url1)\n",
        "text2 = get_text_from_url(url2)\n",
        "print(f\"Extrait du document 1 :\\n{text1[:50]}...\\n\")\n",
        "print(f\"Extrait du document 2 :\\n{text2[:50]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzIvW_facbaU",
        "outputId": "e9d5b720-fccf-4b8a-f239-cc9fa72283f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extrait du document 1 après prétraitement :\n",
            "era generative artificial intelligence ai changing world figuratively literally energy water consumption large technology company main developer techn...\n",
            "\n",
            "Extrait du document 2 après prétraitement :\n",
            "icy cookie policy world continues grapple environmental challenge climate change deforestation pollution growing interest technology help address issu...\n"
          ]
        }
      ],
      "source": [
        "text1 = text1[:len(text1)-870] # Nettoyage du texte\n",
        "text1_processed = preprocess_text(text1)\n",
        "\n",
        "text2 = text2[650:len(text2)-260] # Nettoyage du texte\n",
        "text2_processed = preprocess_text(text2)\n",
        "\n",
        "print(f\"Extrait du document 1 après prétraitement :\\n{text1_processed[:150]}...\\n\")\n",
        "print(f\"Extrait du document 2 après prétraitement :\\n{text2_processed[:150]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IntZzYXroK-9"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxfS5BwfjA6s"
      },
      "source": [
        "## Approche Basée sur la Similarité Lexicale : (TF-IDF, Cosine Similarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xgx9vXhUe_zd",
        "outputId": "76a70675-d7fb-48b1-a1e3-baeb4e6e635a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarité Cosinus entre les deux documents: 0.1951066470124011 \n",
            "\n",
            "Termes significatifs dans Document 1:\n",
            "year : 0.20695517880250144\n",
            "water : 0.20246918206064746\n",
            "data : 0.18406289278240678\n",
            "ai : 0.18406289278240678\n",
            "company : 0.1472503142259254\n",
            "consumption : 0.1472503142259254\n",
            "google : 0.12934698675156342\n",
            "center : 0.12934698675156342\n",
            "data center : 0.12934698675156342\n",
            "increase : 0.12884402494768474\n",
            "\n",
            "--------------------------------------\n",
            "\n",
            "Termes significatifs dans Document 2:\n",
            "ai : 0.4928816962712112\n",
            "help : 0.23090934217811776\n",
            "waste : 0.21380494646122017\n",
            "environmental : 0.20688861324964422\n",
            "used : 0.19471869482319457\n",
            "reduce : 0.17959615502742493\n",
            "potential : 0.1453873635936297\n",
            "system : 0.13386910269094626\n",
            "risk : 0.1282829678767321\n",
            "energy : 0.12778414347772143\n"
          ]
        }
      ],
      "source": [
        "documents = [text1_processed, text2_processed]\n",
        "\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2)) # Initialisation du TfidfVectorizer\n",
        "\n",
        "# Calcul des TF-IDF pour chaque document\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Calcul de la similarité cosinus\n",
        "cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
        "\n",
        "print(\"Similarité Cosinus entre les deux documents:\", cosine_sim[0][0],\"\\n\")\n",
        "\n",
        "# Affichage des termes avec les poids les plus élevés pour chaque document\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "dense = tfidf_matrix.todense()\n",
        "doc1_tfidf = dense[0].tolist()[0]\n",
        "doc2_tfidf = dense[1].tolist()[0]\n",
        "\n",
        "# Obtenir les termes les plus significatifs (top_n)\n",
        "top_n = 10\n",
        "indices1 = np.argsort(doc1_tfidf)[::-1][:top_n]\n",
        "indices2 = np.argsort(doc2_tfidf)[::-1][:top_n]\n",
        "\n",
        "print(\"Termes significatifs dans Document 1:\")\n",
        "for idx in indices1:\n",
        "    print(feature_names[idx], \":\", doc1_tfidf[idx])\n",
        "\n",
        "print(\"\\n--------------------------------------\")\n",
        "\n",
        "print(\"\\nTermes significatifs dans Document 2:\")\n",
        "for idx in indices2:\n",
        "    print(feature_names[idx], \":\", doc2_tfidf[idx])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7GafObyfotw"
      },
      "source": [
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV3FkORzUVp4"
      },
      "source": [
        "## Analyse Sémantique et Similarité Basée sur les Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1LPLpq0xLUX"
      },
      "source": [
        "### Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiNCI0SrUUz-",
        "outputId": "a6b9c9bb-5152-48e1-e325-9bc052d1174f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 3 Paires de paragraphes similaires (Document 1 vs Document 2):\n",
            "Doc1: Shaolei Ren, an associate professor of electrical and computer engineering at the University of California, Riverside and a specialist in AI sustainability, believes it is safe to infer that AI is responsible for this escalation in pollution and resource consumption. \n",
            "Doc2: To maximize the potential benefits of AI and minimize its risks, it is important to develop ethical and responsible AI systems that prioritize environmental protection and sustainability. \n",
            "Score de Similarité: 0.9952789545059204\n",
            "\n",
            "Doc1: The companies only provide data on the water they use to cool the data centers, but do not include in their reports either the water used to generate the electricity they consume or the water used in the supply chain of the products (mainly in the manufacturing of chips and other hardware), as is the case, for example, with carbon emissions. \n",
            "Doc2: The Potential Risks of AI for the Environment While AI has the potential to make a significant positive impact on the environment, there are also potential risks associated with its use. \n",
            "Score de Similarité: 0.9947940707206726\n",
            "\n",
            "Doc1: Shaolei Ren, an associate professor of electrical and computer engineering at the University of California, Riverside and a specialist in AI sustainability, believes it is safe to infer that AI is responsible for this escalation in pollution and resource consumption. \n",
            "Doc2: By leveraging AI to optimize energy systems, reduce waste, and improve resource management, we can create a more sustainable and equitable world for all.” Fei-Fei Li, Co-Director of Stanford University’s Human-Centered AI Institue The Role of AI in Renewable Energy One of the most promising applications of AI in the environmental space is the development of renewable energy systems. \n",
            "Score de Similarité: 0.9944881200790405\n",
            "\n",
            "\n",
            "Top 3 Paires de paragraphes divergents (Document 1 vs Document 2):\n",
            "Doc1: The era of generative artificial intelligence (AI) is changing the world, both figuratively and literally. (Polarité: -0.6)\n",
            "Doc2: The system analyses data from satellites and sensors to create more accurate models of hurricane behaviour, which can help emergency responders prepare for and respond to hurricanes more effectively. (Polarité: 0.5)\n",
            "Différence de Polarité: 1.1\n",
            "\n",
            "Doc1: The era of generative artificial intelligence (AI) is changing the world, both figuratively and literally. (Polarité: -0.6)\n",
            "Doc2: However, the intermittency of these sources can make it challenging to predict and manage energy supply and demand. (Polarité: 0.5)\n",
            "Différence de Polarité: 1.1\n",
            "\n",
            "Doc1: The era of generative artificial intelligence (AI) is changing the world, both figuratively and literally. (Polarité: -0.6)\n",
            "Doc2: The system can provide detailed information on the sources of pollution, which can help policymakers develop targeted strategies to reduce pollution levels. (Polarité: 0.4)\n",
            "Différence de Polarité: 1.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Tokenisation et entraînement du modèle Word2Vec\n",
        "tokenized_docs = [nltk.word_tokenize(doc.lower()) for doc in [text1, text2]]\n",
        "model = Word2Vec(sentences=tokenized_docs, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Fonction pour obtenir l'embedding moyen d'une phrase\n",
        "def get_avg_embedding(text, model):\n",
        "    words = [word for word in nltk.word_tokenize(text.lower()) if word in model.wv]\n",
        "\n",
        "    return np.mean([model.wv[word] for word in words], axis=0)\n",
        "\n",
        "# Fonction pour détecter la polarité avec TextBlob\n",
        "def get_polarity(text):\n",
        "    return TextBlob(text).sentiment.polarity\n",
        "\n",
        "# Diviser les documents en paragraphes/phrases\n",
        "paragraphs_doc1 = nltk.sent_tokenize(text1)\n",
        "paragraphs_doc2 = nltk.sent_tokenize(text2)\n",
        "\n",
        "# Listes pour stocker les résultats\n",
        "divergent_paragraphs = []\n",
        "similar_paragraphs = []\n",
        "\n",
        "# Comparer chaque paragraphe de doc1 avec chaque paragraphe de doc2\n",
        "for para1 in paragraphs_doc1:\n",
        "    embedding_para1 = get_avg_embedding(para1, model)\n",
        "    polarity_para1 = get_polarity(para1)\n",
        "    for para2 in paragraphs_doc2:\n",
        "        embedding_para2 = get_avg_embedding(para2, model)\n",
        "        polarity_para2 = get_polarity(para2)\n",
        "\n",
        "        # Calcul de la similarité cosinus\n",
        "        sim_score = cosine_similarity([embedding_para1], [embedding_para2])[0][0]\n",
        "\n",
        "        # Identifier les paragraphes divergents\n",
        "        if polarity_para1 * polarity_para2 < 0:  # Polarités opposées\n",
        "            polarity_diff = abs(polarity_para1 - polarity_para2)\n",
        "            divergent_paragraphs.append((para1, para2, polarity_para1, polarity_para2, polarity_diff))\n",
        "\n",
        "        # Identifier les paragraphes similaires\n",
        "        if sim_score > 0.7:  # Forte similarité\n",
        "            similar_paragraphs.append((para1, para2, sim_score, polarity_para1, polarity_para2))\n",
        "\n",
        "# Trier les paragraphes similaires par score de similarité et sélectionner les 3 premiers\n",
        "similar_paragraphs_sorted = sorted(similar_paragraphs, key=lambda x: x[2], reverse=True)[:3]\n",
        "\n",
        "# Trier les paragraphes divergents par différence de polarité et sélectionner les 3 premiers\n",
        "divergent_paragraphs_sorted = sorted(divergent_paragraphs, key=lambda x: x[4], reverse=True)[:3]\n",
        "\n",
        "# Afficher les résultats\n",
        "print(\"Top 3 Paires de paragraphes similaires (Document 1 vs Document 2):\")\n",
        "for para1, para2, score, pol1, pol2 in similar_paragraphs_sorted:\n",
        "    print(f\"Doc1: {para1} \\nDoc2: {para2} \\nScore de Similarité: {score}\\n\")\n",
        "\n",
        "print(\"\\nTop 3 Paires de paragraphes divergents (Document 1 vs Document 2):\")\n",
        "for para1, para2, pol1, pol2, pol_diff in divergent_paragraphs_sorted:\n",
        "    print(f\"Doc1: {para1} (Polarité: {pol1})\\nDoc2: {para2} (Polarité: {pol2})\\nDifférence de Polarité: {pol_diff}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsbJkOqyxXi1"
      },
      "source": [
        "### SBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKJZX451Umw4"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DOO3_sgxb7-",
        "outputId": "246b1be4-7865-42ee-9b0f-e7225bc7e52f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 3 Paires de paragraphes similaires (Document 1 vs Document 2):\n",
            "Doc1: “The main driver of the increase in global carbon emissions is that associated with the manufacturing of AI chips and the construction of data centers,” he explains. \n",
            "Doc2: The potential impact of AI on the environment is significant. \n",
            "Score de Similarité: 0.6001753211021423\n",
            "\n",
            "Doc1: The era of generative artificial intelligence (AI) is changing the world, both figuratively and literally. \n",
            "Doc2:  lot of attention is artificial intelligence (AI). \n",
            "Score de Similarité: 0.5985528230667114\n",
            "\n",
            "Doc1: “The main driver of the increase in global carbon emissions is that associated with the manufacturing of AI chips and the construction of data centers,” he explains. \n",
            "Doc2: While AI can help reduce the environmental impact of these industries, it could also enable them to operate more efficiently, leading to increased greenhouse gas emissions and other environmental damage. \n",
            "Score de Similarité: 0.5799895524978638\n",
            "\n",
            "\n",
            "Top 3 Paires de paragraphes divergents (Document 1 vs Document 2):\n",
            "Doc1: The increase is 67% and 40%, respectively, if the last four years are observed. \n",
            "Doc2: Autonomous weapons could be used to target wildlife populations or destroy critical habitats, leading to irreversible damage to the environment. \n",
            "Score de Divergence (ajusté): -0.29418427861332896\n",
            "\n",
            "Doc1: The increase is 67% and 40%, respectively, if the last four years are observed. \n",
            "Doc2: For example, AI could be used to develop autonomous weapons that could be used to harm the environment and wildlife. \n",
            "Score de Divergence (ajusté): -0.2783126201927662\n",
            "\n",
            "Doc1: Google, responsible for the Gemini model, has reported a 16.2% increase in energy consumption in 2023 compared to the previous year. \n",
            "Doc2: Autonomous weapons could be used to target wildlife populations or destroy critical habitats, leading to irreversible damage to the environment. \n",
            "Score de Divergence (ajusté): -0.27700424612045293\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Charger SBERT\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# Diviser les documents en paragraphes\n",
        "paragraphs_doc1 = nltk.sent_tokenize(text1)\n",
        "paragraphs_doc2 = nltk.sent_tokenize(text2)\n",
        "\n",
        "# Calculer les embeddings de chaque paragraphe\n",
        "embeddings_doc1 = model.encode(paragraphs_doc1, convert_to_tensor=True)\n",
        "embeddings_doc2 = model.encode(paragraphs_doc2, convert_to_tensor=True)\n",
        "\n",
        "# Pour l'analyse de polarité\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Listes pour stocker les résultats\n",
        "similar_paragraphs = []\n",
        "divergent_paragraphs = []\n",
        "\n",
        "# Comparer chaque paragraphe de doc1 avec chaque paragraphe de doc2\n",
        "for i, embedding1 in enumerate(embeddings_doc1):\n",
        "    for j, embedding2 in enumerate(embeddings_doc2):\n",
        "        # Calcul de la similarité cosinus\n",
        "        sim_score = util.cos_sim(embedding1, embedding2).item()\n",
        "\n",
        "        # Calcul de la polarité\n",
        "        polarity1 = sia.polarity_scores(paragraphs_doc1[i])['compound']\n",
        "        polarity2 = sia.polarity_scores(paragraphs_doc2[j])['compound']\n",
        "        polarity_diff = abs(polarity1 - polarity2)  # Différence de polarité\n",
        "\n",
        "        # Ajustement du score de divergence avec la polarité\n",
        "        adjusted_divergence_score = sim_score - polarity_diff * 0.2\n",
        "\n",
        "        # Stocker les paires similaires (score élevé)\n",
        "        if sim_score > 0.5:  # Ajustez le seuil si nécessaire\n",
        "            similar_paragraphs.append((paragraphs_doc1[i], paragraphs_doc2[j], sim_score))\n",
        "\n",
        "        # Stocker les paires divergentes (score faible ou contexte opposé)\n",
        "        if adjusted_divergence_score < -0.1:  # Seuil pour divergence\n",
        "            divergent_paragraphs.append((paragraphs_doc1[i], paragraphs_doc2[j], adjusted_divergence_score))\n",
        "\n",
        "similar_paragraphs_sorted = sorted(similar_paragraphs, key=lambda x: x[2], reverse=True)[:3]\n",
        "divergent_paragraphs_sorted = sorted(divergent_paragraphs, key=lambda x: x[2])[:3]\n",
        "\n",
        "# Afficher les résultats\n",
        "print(\"Top 3 Paires de paragraphes similaires (Document 1 vs Document 2):\")\n",
        "for para1, para2, score in similar_paragraphs_sorted:\n",
        "    print(f\"Doc1: {para1} \\nDoc2: {para2} \\nScore de Similarité: {score}\\n\")\n",
        "\n",
        "print(\"\\nTop 3 Paires de paragraphes divergents (Document 1 vs Document 2):\")\n",
        "for para1, para2, score in divergent_paragraphs_sorted:\n",
        "    print(f\"Doc1: {para1} \\nDoc2: {para2} \\nScore de Divergence (ajusté): {score}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcZNJ0QAOEkb"
      },
      "source": [
        "---\n",
        "---\n",
        "\n",
        "## Résumés de Texte et Comparaison Basée sur les Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kqQqG4NOFtD"
      },
      "source": [
        "SUMY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4XC0TQRvM4s"
      },
      "outputs": [],
      "source": [
        "!pip install sumy\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyrZE7mjOES7",
        "outputId": "0a06385e-4dbb-492a-d85a-9c296e376d25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Résumé du Document 1:\n",
            "The energy and water consumption of large technology companies, the main developers of this technology, as well as their carbon emissions, have skyrocketed in recent years.\n",
            "And projections show that the trend will not change.\n",
            "Shaolei Ren, an associate professor of electrical and computer engineering at the University of California, Riverside and a specialist in AI sustainability, believes it is safe to infer that AI is responsible for this escalation in pollution and resource consumption.\n",
            "Microsoft, owner of Copilot and which has lent its infrastructure to OpenAI to develop all versions of ChatGPT and the Dall-E image generator, has recorded a growth of 28.7%, as reflected in its annual sustainability report.\n",
            "Almost the same thing has happened at Google, with an increase of 67% in this period.\n",
            "All this activity has stretched energy demand, to the point that some companies, aware that the trend will continue to rise for some time, are studying developing small nuclear power plants to ensure a sufficient and stable supply.\n",
            "The data centers in which AI (and all digital activity) is operated are large industrial warehouses populated with rows and rows of racks, processors arranged in the shape of a cabinet or refrigerator.\n",
            "Microsoft, for example, has reported using almost 13 billion liters of water.\n",
            "More than half of that volume (about 8 billion liters) was evaporated or consumed, so it could not be reused.\n",
            "Google, for its part, needed less water, about 8.6 billion liters, but only returned 26.6% of that amount to the system.\n",
            "From Google to Microsoft, Meta or Amazon (which have not yet published their environmental reports for this year) and Apple, all the big technology companies are immersed in programs to improve their carbon emissions records and reduce the amount of water they use.\n",
            "The goal for many of them is to reach 2030 with a very low environmental footprint.\n",
            "That same year, the energy demand of AI will be between 85 and 134 TWh.\n",
            "“If we only look at the emissions derived from their direct energy and water consumption, they can achieve no emissions or not use more water than they contribute by 2030, perhaps even sooner,” concludes Ren.\n",
            "“But if we look at their real footprint, it is quite unlikely that they will achieve neutrality by 2030.” Sig\n",
            "\n",
            "Résumé du Document 2:\n",
            "It can also be used to optimize energy consumption, reduce waste, and improve the efficiency of transportation systems.\n",
            "This information can be used to track the progress of environmental initiatives, identify areas that need improvement, and inform policy decisions.\n",
            "For example, the National Oceanic and Atmospheric Administration (NOAA) is using AI to improve hurricane forecasting.\n",
            "The company has developed an AI-powered system that can help commercial kitchens reduce food waste by up to 50%.\n",
            "For example, the World Wildlife Fund is using AI to track animal activity in protected areas.\n",
            "In addition, AI can be used to optimize the design and operation of renewable energy systems.\n",
            "AI can help address these challenges by optimizing agricultural practices to reduce waste and increase efficiency.\n",
            "Finally, there is a risk that AI could be used to exacerbate existing environmental injustices.\n",
            "For example, the development and deployment of AI technologies should be guided by principles of transparency, accountability, and fairness to ensure that they are used in ways that are beneficial to society as a whole.\n",
            "One important area where AI can help address environmental challenges is in climate modeling and prediction.\n",
            "In agriculture, AI can help optimize crop yields while reducing the environmental impact of farming practises.\n",
            "However, there are also potential risks associated with the use of AI in these areas.\n",
            "For example, there is a risk that AI algorithms could be biased or inaccurate, leading to unintended environmental consequences.\n",
            "Additionally, the use of AI in these areas could lead to job displacement and economic inequality if not managed carefully.\n",
            "AI technologies can help improve renewable energy systems, optimise agriculture, reduce waste, and monitor environmental conditions.\n"
          ]
        }
      ],
      "source": [
        "def summarize_text(input_text, sentences_count):\n",
        "    # Parser le texte d'entrée\n",
        "    parser = PlaintextParser.from_string(input_text, Tokenizer(\"english\"))\n",
        "    # Créer un résumé LSA\n",
        "    summarizer = LsaSummarizer()\n",
        "    # Générer le résumé\n",
        "    summary = summarizer(parser.document, sentences_count)\n",
        "    return summary\n",
        "\n",
        "summary_text1 = summarize_text(text1, sentences_count=15)\n",
        "summary_text2 = summarize_text(text2, sentences_count=15)\n",
        "\n",
        "# Afficher les résumés\n",
        "print(\"Résumé du Document 1:\")\n",
        "for sentence in summary_text1:\n",
        "    print(sentence)\n",
        "\n",
        "print(\"\\nRésumé du Document 2:\")\n",
        "for sentence in summary_text2:\n",
        "    print(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3xe7yr6xVau",
        "outputId": "ddd3b5cb-10a5-40e9-c122-68da6af2daec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 3 Paires de paragraphes similaires (Document 1 vs Document 2):\n",
            "Doc1: Shaolei Ren, an associate professor of electrical and computer engineering at the University of California, Riverside and a specialist in AI sustainability, believes it is safe to infer that AI is responsible for this escalation in pollution and resource consumption. \n",
            "Doc2: Finally, there is a risk that AI could be used to exacerbate existing environmental injustices. \n",
            "Score de Similarité: 0.5769515633583069\n",
            "\n",
            "Doc1: Shaolei Ren, an associate professor of electrical and computer engineering at the University of California, Riverside and a specialist in AI sustainability, believes it is safe to infer that AI is responsible for this escalation in pollution and resource consumption. \n",
            "Doc2: For example, there is a risk that AI algorithms could be biased or inaccurate, leading to unintended environmental consequences. \n",
            "Score de Similarité: 0.5259228944778442\n",
            "\n",
            "Doc1: That same year, the energy demand of AI will be between 85 and 134 TWh. \n",
            "Doc2: In addition, AI can be used to optimize the design and operation of renewable energy systems. \n",
            "Score de Similarité: 0.5121006965637207\n",
            "\n",
            "\n",
            "Top 3 Paires de paragraphes divergents (Document 1 vs Document 2):\n",
            "Doc1: More than half of that volume (about 8 billion liters) was evaporated or consumed, so it could not be reused. \n",
            "Doc2: For example, the World Wildlife Fund is using AI to track animal activity in protected areas. \n",
            "Score de Divergence: -0.1040085107088089\n",
            "\n",
            "Doc1: And projections show that the trend will not change. \n",
            "Doc2: The company has developed an AI-powered system that can help commercial kitchens reduce food waste by up to 50%. \n",
            "Score de Divergence: -0.08132528513669968\n",
            "\n",
            "Doc1: More than half of that volume (about 8 billion liters) was evaporated or consumed, so it could not be reused. \n",
            "Doc2: This information can be used to track the progress of environmental initiatives, identify areas that need improvement, and inform policy decisions. \n",
            "Score de Divergence: -0.036256492137908936\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Joindre le texte résumé en une seule chaîne\n",
        "sum_text1 = \" \".join([str(sentence) for sentence in summary_text1])\n",
        "sum_text2 = \" \".join([str(sentence) for sentence in summary_text2])\n",
        "\n",
        "# Charger le modèle SBERT\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# Diviser les documents en paragraphes\n",
        "paragraphs_doc1 = nltk.sent_tokenize(sum_text1)\n",
        "paragraphs_doc2 = nltk.sent_tokenize(sum_text2)\n",
        "\n",
        "# Calculer les embeddings pour chaque paragraphe\n",
        "embeddings_doc1 = model.encode(paragraphs_doc1, convert_to_tensor=True)\n",
        "embeddings_doc2 = model.encode(paragraphs_doc2, convert_to_tensor=True)\n",
        "\n",
        "# Listes pour stocker les résultats\n",
        "similar_paragraphs = []\n",
        "divergent_paragraphs = []\n",
        "\n",
        "# Comparer chaque paragraphe de doc1 avec chaque paragraphe de doc2\n",
        "for i, embedding1 in enumerate(embeddings_doc1):\n",
        "    for j, embedding2 in enumerate(embeddings_doc2):\n",
        "        # Calcul de la similarité cosinus\n",
        "        sim_score = util.cos_sim(embedding1, embedding2).item()\n",
        "\n",
        "        # Stocker les paires similaires (cosinus élevé)\n",
        "        if sim_score > 0.4:  # Ajustez le seuil si nécessaire\n",
        "            similar_paragraphs.append((paragraphs_doc1[i], paragraphs_doc2[j], sim_score))\n",
        "\n",
        "        # Stocker les paires divergentes (cosinus négatif)\n",
        "        if sim_score < 0:  # Seuil pour divergence\n",
        "            divergent_paragraphs.append((paragraphs_doc1[i], paragraphs_doc2[j], sim_score))\n",
        "\n",
        "# Trier les paragraphes similaires et divergents par score\n",
        "similar_paragraphs_sorted = sorted(similar_paragraphs, key=lambda x: x[2], reverse=True)[:3]\n",
        "divergent_paragraphs_sorted = sorted(divergent_paragraphs, key=lambda x: x[2])[:3]\n",
        "\n",
        "# Afficher les résultats\n",
        "print(\"Top 3 Paires de paragraphes similaires (Document 1 vs Document 2):\")\n",
        "for para1, para2, score in similar_paragraphs_sorted:\n",
        "    print(f\"Doc1: {para1} \\nDoc2: {para2} \\nScore de Similarité: {score}\\n\")\n",
        "\n",
        "print(\"\\nTop 3 Paires de paragraphes divergents (Document 1 vs Document 2):\")\n",
        "for para1, para2, score in divergent_paragraphs_sorted:\n",
        "    print(f\"Doc1: {para1} \\nDoc2: {para2} \\nScore de Divergence: {score}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JRCY7Hz3Gtw"
      },
      "source": [
        "## Approche Basée un Large Language Model (LLM) :\n",
        "\n",
        "Nous utilisons le modèle pré-entraîné **Qwen/Qwen2.5-1.5B-Instruct** disponible dans la bibliothèque Hugging Face Transformers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWPB_qy23J-E",
        "outputId": "45a9c613-ccc7-47e6-fbd3-db52d5780825"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "! huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Dkig92tg4LEr"
      },
      "outputs": [],
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUl2yaLR6_68",
        "outputId": "f77883c9-92f9-4ac2-85f3-d729fe580c5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Compare the following two texts and highlight their common ideas as well as their differences:\n",
            "\n",
            "Text 1: The energy and water consumption of large technology companies, the main developers of this technology, as well as their carbon emissions, have skyrocketed in recent years. And projections show that the trend will not change. Shaolei Ren, an associate professor of electrical and computer engineering at the University of California, Riverside and a specialist in AI sustainability, believes it is safe to infer that AI is responsible for this escalation in pollution and resource consumption. Microsoft, owner of Copilot and which has lent its infrastructure to OpenAI to develop all versions of ChatGPT and the Dall-E image generator, has recorded a growth of 28.7%, as reflected in its annual sustainability report. Almost the same thing has happened at Google, with an increase of 67% in this period. All this activity has stretched energy demand, to the point that some companies, aware that the trend will continue to rise for some time, are studying developing small nuclear power plants to ensure a sufficient and stable supply. The data centers in which AI (and all digital activity) is operated are large industrial warehouses populated with rows and rows of racks, processors arranged in the shape of a cabinet or refrigerator. Microsoft, for example, has reported using almost 13 billion liters of water. More than half of that volume (about 8 billion liters) was evaporated or consumed, so it could not be reused. Google, for its part, needed less water, about 8.6 billion liters, but only returned 26.6% of that amount to the system. From Google to Microsoft, Meta or Amazon (which have not yet published their environmental reports for this year) and Apple, all the big technology companies are immersed in programs to improve their carbon emissions records and reduce the amount of water they use. The goal for many of them is to reach 2030 with a very low environmental footprint. That same year, the energy demand of AI will be between 85 and 134 TWh. “If we only look at the emissions derived from their direct energy and water consumption, they can achieve no emissions or not use more water than they contribute by 2030, perhaps even sooner,” concludes Ren. “But if we look at their real footprint, it is quite unlikely that they will achieve neutrality by 2030.” Sig\n",
            "\n",
            "Text 2: It can also be used to optimize energy consumption, reduce waste, and improve the efficiency of transportation systems. This information can be used to track the progress of environmental initiatives, identify areas that need improvement, and inform policy decisions. For example, the National Oceanic and Atmospheric Administration (NOAA) is using AI to improve hurricane forecasting. The company has developed an AI-powered system that can help commercial kitchens reduce food waste by up to 50%. For example, the World Wildlife Fund is using AI to track animal activity in protected areas. In addition, AI can be used to optimize the design and operation of renewable energy systems. AI can help address these challenges by optimizing agricultural practices to reduce waste and increase efficiency. Finally, there is a risk that AI could be used to exacerbate existing environmental injustices. For example, the development and deployment of AI technologies should be guided by principles of transparency, accountability, and fairness to ensure that they are used in ways that are beneficial to society as a whole. One important area where AI can help address environmental challenges is in climate modeling and prediction. In agriculture, AI can help optimize crop yields while reducing the environmental impact of farming practises. However, there are also potential risks associated with the use of AI in these areas. For example, there is a risk that AI algorithms could be biased or inaccurate, leading to unintended environmental consequences. Additionally, the use of AI in these areas could lead to job displacement and economic inequality if not managed carefully. AI technologies can help improve renewable energy systems, optimise agriculture, reduce waste, and monitor environmental conditions..\n",
            " I want the result in this format \n",
            "Common ideas:\n",
            "\n",
            "Differences: Text 1 focuses on the increasing pollution caused by large technology companies due to AI's high energy and water consumption, whereas Text 2 highlights how AI can be used to optimize energy consumption, reduce waste, and improve the efficiency of transportation systems.\n",
            "\n",
            "The text discusses the growing concern over the excessive energy consumption and water usage of large technology companies, particularly those involved in the development and operation of AI systems. These companies are projected to experience significant increases in their energy demands, leading to concerns about the sustainability of such activities. Professor Shaolei Ren suggests that AI may be partly responsible for this increased pollution and resource consumption. Major tech firms like Microsoft and Google have seen substantial growth in their environmental reporting, reflecting the scale of their operations. While both texts emphasize the importance of addressing these issues through improved sustainability measures, Text 2 goes further, exploring how AI can be leveraged to enhance various aspects of environmental management, including energy optimization, waste reduction, and renewable energy integration. \n",
            "\n",
            "In summary, Text 1 primarily addresses the adverse effects of AI on environmental sustainability, focusing on the rising pollution levels linked to technological advancements. On the other hand, Text 2 delves deeper into the multifaceted role of AI in promoting sustainable practices across different sectors, highlighting its capacity to mitigate environmental impacts effectively. Both texts underscore the urgency of integrating AI solutions into environmental strategies to safeguard our planet.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "def compare_texts(text1, text2):\n",
        "\n",
        "    prompt = f\"Compare the following two texts and highlight their common ideas as well as their differences:\\n\\nText 1: {text1}\\n\\nText 2: {text2}.\\n I want the result in this format \\nCommon ideas:\\n\\nDifferences:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)  # Envoyer les entrées sur le même périphérique que le modèle\n",
        "    outputs = model.generate(inputs['input_ids'], max_length=2000)  # Utiliser les ID d'entrée uniquement\n",
        "    comparison = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return comparison\n",
        "\n",
        "result = compare_texts(sum_text1, sum_text2)\n",
        "print(result)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
